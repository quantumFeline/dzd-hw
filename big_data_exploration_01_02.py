# -*- coding: utf-8 -*-
"""Big Data Exploration - 01/02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sj7PJCsZFktqZGZXOVjZsJZleVTx4d48

# Lab 01 / 02 - Techniques for Handling Large Datasets in Classification; K-means; K-medoids

This homework is an exploration of the first two labs from DZD lessons, pertaining to dataset processing, feature engineering, and K-means/K-medoids clustering.

## Data cleanup

We will use the [NYC Yellow Taxi Trip Data](https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data) from Kaggle.
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("elemento/nyc-yellow-taxi-trip-data")

print("Path to dataset files:", path)

import pandas as pd

taxi = pd.read_csv(path + "/yellow_tripdata_2015-01.csv")
taxi.head()

import numpy as np
import matplotlib.pyplot as plt

"""First we will filter away the garbage data. Some indicators of garbage data are:
* Pickup later than dropoff (taxis don't travel in time!)
* Zero distance
* Fare amount below minimum in in NYC
* Passenger count above 12 (most taxis, even large ones, can fit 6 max, I figured I can be generous)
* Pickups and dropoffs way outside of NYC

(I asked my NYC-dwelling friends about some of these.)
"""

print("Raw:", len(taxi))
mask = (
    (taxi["tpep_pickup_datetime"] <= taxi["tpep_dropoff_datetime"]) &
    (taxi["trip_distance"] > 0) &
    (taxi["fare_amount"] >= 2.5) & # min fare in NYC
    (taxi["passenger_count"].between(1, 12)) &
    taxi["pickup_latitude"].between(40.0, 41.5) & # NYC & boroughs & airports
    taxi["pickup_longitude"].between(-75.0, -72.0) &
    taxi["dropoff_latitude"].between(40.0, 41.5) & # NYC & boroughs & airports
    taxi["dropoff_longitude"].between(-75.0, -72.0)
)
clean_taxi = taxi[mask].copy()
print("After removing nonsensical data:", len(clean_taxi))
print("Percentage:", len(clean_taxi)/len(taxi))

"""We filtered out about 3% of our data. Let's do a few more iterations of this, starting with removing significant outliers."""

import gc

del taxi  # free up memory

gc.collect()

q_high = clean_taxi["trip_distance"].quantile(0.99)
filtered_taxi = clean_taxi[clean_taxi["trip_distance"] < q_high].copy()

plt.figure(figsize=(10, 6))
plt.hist(filtered_taxi["trip_distance"], bins=50, edgecolor='black')
plt.xlabel('Trip Distance (miles)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
print(f"After removing outliers: {len(filtered_taxi)}")

print(q_high)

del clean_taxi  # free up memory

gc.collect()

"""Check the new coordinates range just to be sure:"""

print("Latitude range:", filtered_taxi["pickup_latitude"].min(), filtered_taxi["pickup_latitude"].max())
print("Logitude range:", filtered_taxi["dropoff_longitude"].min(), filtered_taxi["dropoff_longitude"].max())

filtered_taxi[["pickup_latitude", "pickup_longitude", "dropoff_latitude", "dropoff_longitude"]].describe()

filtered_taxi["passenger_count"].value_counts()

"""Here we confirm that the data overwhelmingly doesn't have rides with passengers above 6. We could filter out the remaining ones.

We will also start engineer new features, starting with converting the datetame into the proper numeric format as well as measuring the total trip duration.
"""

filtered_taxi["dropoff_datetime"] = pd.to_datetime(filtered_taxi["tpep_dropoff_datetime"])
filtered_taxi["pickup_datetime"] = pd.to_datetime(filtered_taxi["tpep_pickup_datetime"])

duration = (filtered_taxi["dropoff_datetime"] - filtered_taxi["pickup_datetime"]).dt.total_seconds() / 60
print(filtered_taxi[duration < 1].size,filtered_taxi[duration > 120].size)

print(len(filtered_taxi[duration < 1]),len(filtered_taxi[duration > 120]))

f2_taxi = filtered_taxi[(duration > 1) & (duration < 120) & filtered_taxi["passenger_count"] <= 6].copy()
duration = (f2_taxi["dropoff_datetime"] - f2_taxi["pickup_datetime"]).dt.total_seconds() / 60 # recompute so that the indices align
print(f"Rows: {len(f2_taxi)}")  # or f2_taxi.shape[0]
print(f"Total elements: {f2_taxi.size}")  # rows Ã— columns

"""We're down to 96% of the original dataset."""

print(len(f2_taxi))
del filtered_taxi
gc.collect()

f2_taxi.head()

"""## Feature engineering (and more cleanup)

Let's calculate geographical distance of the trip as well (as opposed to the distance on the meter). (Haversine may have been a bit of an overkill, but we figured there's no harm in it.)
"""

from math import radians, sin, cos, sqrt, atan2

def haversine(lat1, lon1, lat2, lon2):
    R = 3959  # Earth radius in miles
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.atan2(np.sqrt(a), np.sqrt(1-a))
    return R * c

f2_taxi["fare_per_mile"] = f2_taxi["fare_amount"] / f2_taxi['trip_distance']
f2_taxi["geo_distance"] = haversine(
    f2_taxi["pickup_latitude"],
    f2_taxi["pickup_longitude"],
    f2_taxi["dropoff_latitude"],
    f2_taxi["dropoff_longitude"]
)
f2_taxi.head()

"""A lot of these features were engineered just to look for any interesting patterns in the displayed dataset subset; we leave them in the final version, but do not include a detailed analysis."""

f2_taxi["hour"] = f2_taxi["pickup_datetime"].dt.hour
f2_taxi["day_of_week"] = f2_taxi["pickup_datetime"].dt.dayofweek
f2_taxi["is_weekend"] = f2_taxi["day_of_week"].isin([5, 6])
f2_taxi["is_rush_hour"] = f2_taxi["hour"].between(7, 9) | f2_taxi["hour"].between(16, 18)
f2_taxi["morning_rush"] = f2_taxi["hour"].between(7, 9)
f2_taxi["evening_rush"] = f2_taxi["hour"].between(16, 18)
f2_taxi["speed_mph"] = f2_taxi["trip_distance"] / (duration / 60)
f2_taxi["route_efficiency"] = np.where(
    f2_taxi["geo_distance"] > 0,
    f2_taxi["trip_distance"] / f2_taxi["geo_distance"],
    np.nan
)
f2_taxi.head()

print(len(f2_taxi[f2_taxi['route_efficiency'] > 20]))
f2_taxi[f2_taxi['route_efficiency'] > 20].head()

f2_taxi[f2_taxi["trip_distance"] < 0.1]["fare_per_mile"].describe()

print((f2_taxi["trip_distance"] > 0.1)[:5], (f2_taxi['route_efficiency'] < 20)[:5])
print(len(f2_taxi["trip_distance"] > 0.1), len(f2_taxi['route_efficiency'] < 20))

"""Let's filter out a few more anomalies - oddly short trips and oddly geographically inefficient routes (which possible include multiple stops and keeping the taxi driver waiting and thus are harder to analyse):"""

f3_taxi = f2_taxi[
    (f2_taxi["trip_distance"] > 0.1) &
    (f2_taxi['route_efficiency'] < 20)
]
print(len(f3_taxi))
f3_taxi.head()

del f2_taxi
gc.collect()

"""This should ensure a high-quality dataset.

Now that we have clean data, we will work with a smaller sample - though large enough for clusters to be of significant size.
"""

taxi_batch = f3_taxi.sample(10000)

"""## Geographical and time data - unsupervised learning

Let's test the following hypothesis: the traffic usually follows stable geographical patterns. In the morning, people commute to work, to the city center; in the evening the travel home from work, away from the center. This means that geographical data may give us potential information about whether it a part of the morning rush or the evening rush.

Let's engineer the features for it - using the coordinates of the city center as our proxy whether the taxi is heading to or from the business district.
"""

taxi_batch_geo = taxi_batch[["pickup_latitude", "pickup_longitude", "dropoff_latitude", "dropoff_longitude", "pickup_datetime", "dropoff_datetime", "is_weekend", "is_rush_hour", "morning_rush", "evening_rush"]].copy()
# Manhattan business district: 40.74-40.76, lon -73.98 to -74.00
taxi_batch_geo["from_business_district"] = taxi_batch_geo["pickup_latitude"].between(40.74, 40.76) & taxi_batch_geo["pickup_longitude"].between(-74.00, -73.98)
taxi_batch_geo["to_business_district"] = taxi_batch_geo["dropoff_latitude"].between(40.74, 40.76) & taxi_batch_geo["dropoff_longitude"].between(-74.00, -73.98)

# dropoff close to midpoint than pickup
midpoint = ((40.75 + 40.74) / 2, (-74.00 + -73.98) / 2)
taxi_batch_geo["headed_downtown"] = np.linalg.norm(taxi_batch_geo[["pickup_latitude", "pickup_longitude"]] - midpoint, axis=1) < np.linalg.norm(taxi_batch_geo[["dropoff_latitude", "dropoff_longitude"]] - midpoint, axis=1)
taxi_batch_geo.head()

"""We also need to check the clusters are large enough for our analysis to make sense - if they are too small of a share in the dataset, we will likely not be able to find meaningful correlations with the standard k-means."""

print("From business district:", taxi_batch_geo["from_business_district"].sum(), "out of", len(taxi_batch_geo))
print("To business district:", taxi_batch_geo["to_business_district"].sum(), "out of", len(taxi_batch_geo))

"""10-15% of the dataset is good enough. As for the clusters,"""

print("Morning rush:", taxi_batch_geo["morning_rush"].sum(), "out of", len(taxi_batch_geo))
print("Evening rush:", taxi_batch_geo["evening_rush"].sum(), "out of", len(taxi_batch_geo))

"""That should give us three uneven, but reasonably sized clusters.

Let's engineer the data and the clusters: we collect the geographical features an try to predict whether a given trip happens during the morning rush hours, evening rush hours, or neither.
"""

taxi_geo_x = taxi_batch_geo[["pickup_latitude", "pickup_longitude", "dropoff_latitude", "dropoff_longitude", "is_weekend", "from_business_district", "to_business_district"]]
taxi_geo_x.head()

taxi_geo_y = taxi_batch_geo.apply(
    lambda x: 0 if x["morning_rush"] else 1 if x["evening_rush"] else 2,
    axis=1
)
taxi_geo_y.head()

print(np.sum(taxi_geo_y == 0), np.sum(taxi_geo_y == 1), np.sum(taxi_geo_y == 2))

"""Of course, we should normalize out features before feeding them into the k-means algorithm:"""

taxi_geo_x_means = taxi_geo_x.mean()
taxi_geo_x_stds = taxi_geo_x.std()
taxi_geo_x_normalized = (taxi_geo_x - taxi_geo_x_means) / taxi_geo_x_stds
taxi_geo_x_normalized.head()

taxi_geo_x_normalized.describe()

"""The K-means implementation is standard:"""

def k_means(x, n_clusters=3, max_iter=100):
    centers = x.sample(n_clusters).values
    print("Centers:", centers)

    for _ in range(max_iter):
        distances = np.sqrt(((x.values - centers[:, np.newaxis])**2).sum(axis=2))
        #print("Distances:", distances)
        labels = np.argmin(distances, axis=0)
        #print("Labels:", labels)

        new_centers = np.array([x.values[labels == i].mean(axis=0) for i in range(n_clusters)])

        # compute the average between all values

        if np.allclose(centers, new_centers):
            break

        centers = new_centers

    return labels, centers

init_labels, init_centers = k_means(taxi_geo_x_normalized, n_clusters=3, max_iter=1)
print("Initial cluster sizes:", np.sum(init_labels == 0), np.sum(init_labels == 1), np.sum(init_labels == 2))
print("Initial average distance:", np.mean(np.sqrt(((taxi_geo_x_normalized.values - init_centers[:, np.newaxis])**2).sum(axis=2))))

labels, centers = k_means(taxi_geo_x_normalized, n_clusters=3)
print("Cluster sizes:", np.sum(labels == 0), np.sum(labels == 1), np.sum(labels == 2))
print("average distance:", np.mean(np.sqrt(((taxi_geo_x_normalized.values - centers[:, np.newaxis])**2).sum(axis=2))))

"""Let's measure the performance. With three clusters and unsupervised learning, we have to either draw confusion matrices or measure efficiency in some other way. The two classical measures are similarity between clusters (Rand index, from 0 to 1) or dependence (NMI, from -1 to 1)."""

from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

ari = adjusted_rand_score(taxi_geo_y, labels)
nmi = normalized_mutual_info_score(taxi_geo_y, labels)

print("Adjusted Rand Index (ARI):", ari)
print("Normalized Mutual Information (NMI):", nmi)

"""This is a pretty poor result. May a more sophisticated algrothim perform better?"""

import umap

import numpy as np
import pandas as pd

def k_medoids(x, n_clusters=2, max_iter=100):
    x_np = x.values if isinstance(x, pd.DataFrame) else x

    medoid_indices = np.random.choice(len(x_np), n_clusters, replace=False)
    medoids = x_np[medoid_indices]

    for iteration in range(max_iter):
        # x_np: (n_samples, n_features)
        # medoids: (n_clusters, n_features)
        # distance: (n_samples, n_clusters)
        distances = np.sqrt(((x_np[:, np.newaxis, :] - medoids[np.newaxis, :, :])**2).sum(axis=2))

        # choose closest
        labels = np.argmin(distances, axis=1)

        # find new medoids
        new_medoids = np.empty_like(medoids)
        for i in range(n_clusters):
            cluster_points = x_np[labels == i]
            if len(cluster_points) == 0:
                # If a cluster becomes empty, keep the old medoid or reinitialize it randomly
                # This can happen if a medoid is an outlier
                new_medoids[i] = medoids[i]
                continue

            min_cost = np.inf
            best_medoid_candidate = None

            for p_candidate in cluster_points:
                # Calculate distances from p_candidate to all other points in the cluster
                cost = np.sum(np.sqrt(((cluster_points - p_candidate)**2).sum(axis=1)))
                if cost < min_cost:
                    min_cost = cost
                    best_medoid_candidate = p_candidate
            new_medoids[i] = best_medoid_candidate

        # convergence Check
        if np.allclose(medoids, new_medoids):
            print(f"K-Medoids converged after {iteration + 1} iterations.")
            break
        medoids = new_medoids

    final_distances = np.sqrt(((x_np[:, np.newaxis, :] - medoids[np.newaxis, :, :])**2).sum(axis=2))
    final_labels = np.argmin(final_distances, axis=1)

    return final_labels, medoids

print("Initial run with:")
init_labels_medoids, init_medoids = k_medoids(taxi_geo_x_normalized, n_clusters=3, max_iter=1)
print("Initial Medoids:\n", init_medoids)
print(f"Cluster 0 size: {np.sum(init_labels_medoids == 0)}, Cluster 1 size: {np.sum(init_labels_medoids == 1)}, Cluster 2 size: {np.sum(init_labels_medoids == 2)}")

init_distances_to_medoids = np.min(np.sqrt(((taxi_geo_x_normalized.values[:, np.newaxis, :] - init_medoids[np.newaxis, :, :])**2).sum(axis=2)), axis=1)
print("Average distance to medoids (initial state):", np.mean(init_distances_to_medoids))

print("\nRunning K-Medoids for full convergence:")
labels_medoids, medoids = k_medoids(taxi_geo_x_normalized, n_clusters=3)
print("Final Medoids:\n", medoids)
print(f"Cluster 0 size: {np.sum(labels_medoids == 0)}, Cluster 1 size: {np.sum(labels_medoids == 1)}, Cluster 2 size: {np.sum(labels_medoids == 2)}")

final_distances_to_medoids = np.min(np.sqrt(((taxi_geo_x_normalized.values[:, np.newaxis, :] - medoids[np.newaxis, :, :])**2).sum(axis=2)), axis=1)
print("Average distance to medoids (final state):", np.mean(final_distances_to_medoids))

ari_medoids = adjusted_rand_score(taxi_geo_y, labels_medoids)
nmi_medoids = normalized_mutual_info_score(taxi_geo_y, labels_medoids)

print("Adjusted Rand Index (ARI):", ari)
print("Normalized Mutual Information (NMI):", nmi)

"""Not much better. Let's visualize our clusters to see if there are any significant abnormalities.

## Results analysis
"""

import umap

reducer = umap.UMAP(n_components=2, random_state=42)
embedding_k_means = reducer.fit_transform(taxi_geo_x_normalized)
embedding_k_medoids = reducer.fit_transform(taxi_geo_x_normalized)

plt.figure(figsize=(15, 6))

# K-Means visualization
plt.subplot(1, 2, 1)
plt.scatter(embedding_k_means[:, 0], embedding_k_means[:, 1], c=labels, cmap='viridis', s=10, alpha=0.6)
plt.title('K-Means Clusters (UMAP Reduction)')
plt.xlabel('UMAP Dimension 1')
plt.ylabel('UMAP Dimension 2')
plt.colorbar(label='Cluster Label')

# K-Medoids visualization
plt.subplot(1, 2, 2)
plt.scatter(embedding_k_medoids[:, 0], embedding_k_medoids[:, 1], c=labels_medoids, cmap='plasma', s=10, alpha=0.6)
plt.title('K-Medoids Clusters (UMAP Reduction)')
plt.xlabel('UMAP Dimension 1')
plt.ylabel('UMAP Dimension 2')
plt.colorbar(label='Cluster Label')

plt.tight_layout()
plt.show()

"""Despite nicely-looking clusters, the ARI and NMI results are poor. Let's visualize our data to figure out why. The end clusters are very similar to each other according to ARI, so perhaps our geographical data just predicts our time data poorly?"""

print(pd.crosstab(taxi_batch_geo["to_business_district"], taxi_geo_y))
print(pd.crosstab(taxi_batch_geo["from_business_district"], taxi_geo_y))

"""This confirms it, but let's calculate the percentage and visualize the geographical data:"""

morning_trips = taxi_batch_geo[(taxi_batch_geo["morning_rush"]) & (~taxi_batch_geo["is_weekend"])]
evening_trips = taxi_batch_geo[(taxi_batch_geo["evening_rush"]) & (~taxi_batch_geo["is_weekend"])]

print(f"Morning rush trips (weekday 7-9 AM): {len(morning_trips)}")
print(f"Evening rush trips (weekday 4-6 PM): {len(evening_trips)}")

print("\n--- Business District Flow ---")
print(f"Morning: TO business district: {morning_trips['to_business_district'].mean():.1%}, FROM: {morning_trips['from_business_district'].mean():.1%}")
print(f"Evening: TO business district: {evening_trips['to_business_district'].mean():.1%}, FROM: {evening_trips['from_business_district'].mean():.1%}")

print(f"\nMorning headed downtown: {morning_trips['headed_downtown'].mean():.1%}")
print(f"Evening headed downtown: {evening_trips['headed_downtown'].mean():.1%}")

# morning vs evening
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

def set_labels(ax):
  ax.set_xlim(-74.20, -73.5)
  ax.set_ylim(40.6, 40.9)
  ax.set_xlabel('Longitude')
  ax.set_ylabel('Latitude')

# Morning pickups
axes[0, 0].scatter(morning_trips["pickup_longitude"], morning_trips["pickup_latitude"],
                   alpha=0.3, s=5, c='blue')
axes[0, 0].set_title(f'Morning pickups (n={len(morning_trips)})')
set_labels(axes[0,0])

# Evening pickups
axes[0, 1].scatter(evening_trips["pickup_longitude"], evening_trips["pickup_latitude"],
                   alpha=0.3, s=5, c='orange')
axes[0, 1].set_title(f'Evening pickups (n={len(evening_trips)})')
set_labels(axes[0,1])

# Morning dropoffs
axes[1, 0].scatter(morning_trips["dropoff_longitude"], morning_trips["dropoff_latitude"],
                   alpha=0.3, s=5, c='blue')
axes[1, 0].set_title(f'Morning dropoffs (n={len(morning_trips)})')
set_labels(axes[1,0])

# Evening dropoffs
axes[1, 1].scatter(evening_trips["dropoff_longitude"], evening_trips["dropoff_latitude"],
                   alpha=0.3, s=5, c='orange')
axes[1, 1].set_title(f'Evening dropoffs (n={len(evening_trips)})')
set_labels(axes[1,1])

plt.tight_layout()
plt.show()

"""Likely conclusion" daily commuters don't use taxis, and tourists don't care much for the business districts.

## More hypotheses for the dataset

Let's try exploring more to find whether there is anything we can predict more reliably from the data. How about airport trips?
"""

# Airport zones
jfk = (40.64, -73.78)
lga = (40.77, -73.87)

taxi_batch["near_jfk"] = ((taxi_batch["pickup_latitude"] - 40.64).abs() < 0.03) | \
                          ((taxi_batch["dropoff_latitude"] - 40.64).abs() < 0.03)

taxi_batch["near_jfk"] = (
    (taxi_batch["pickup_latitude"].between(40.62, 40.66) & taxi_batch["pickup_longitude"].between(-73.82, -73.76)) |
    (taxi_batch["dropoff_latitude"].between(40.62, 40.66) & taxi_batch["dropoff_longitude"].between(-73.82, -73.76))
)
taxi_batch["near_lga"] = (
    (taxi_batch["pickup_latitude"].between(40.76, 40.78) & taxi_batch["pickup_longitude"].between(-73.89, -73.85)) |
    (taxi_batch["dropoff_latitude"].between(40.76, 40.78) & taxi_batch["dropoff_longitude"].between(-73.89, -73.85))
)
taxi_batch["airport_trip"] = taxi_batch["near_jfk"] | taxi_batch["near_lga"]

airport = taxi_batch[taxi_batch["airport_trip"]]
local = taxi_batch[~taxi_batch["airport_trip"]]

print(f"Airport trips: {len(airport)}, Local trips: {len(local)}")
print(f"Airport avg fare: ${airport['fare_amount'].mean():.2f}, distance: {airport['trip_distance'].mean():.1f} mi")
print(f"Local avg fare: ${local['fare_amount'].mean():.2f}, distance: {local['trip_distance'].mean():.1f} mi")

"""Looks like airport trips are more expensive, but perhaps there aren't enough of them for k-means to perform realiable. What about nighttime/daytime pickups - would they have more significant differences than the rush hour trips (entertainment districts etc?)"""

late_night = taxi_batch[taxi_batch["hour"].between(1, 4)]
daytime = taxi_batch[taxi_batch["hour"].between(10, 16)]

print(f"Late night (1-4 AM): {len(late_night)}, Daytime (10 AM-4 PM): {len(daytime)}")

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(late_night["pickup_longitude"], late_night["pickup_latitude"], alpha=0.3, s=5)
axes[0].set_title(f'Late Night Pickups (n={len(late_night)})')
axes[1].scatter(daytime["pickup_longitude"], daytime["pickup_latitude"], alpha=0.3, s=5, c='green')
axes[1].set_title(f'Daytime Pickups (n={len(daytime)})')
plt.tight_layout()
plt.show()

"""# Unsupervised exploration

If we cluster by coordinates only using our k-means, can we spot anything interesting in the data?

(The following code was re-run several times with different cluster size. We only leave N=3 clusters version as the most productive one.)
"""

# Cluster only by coordinates
N_CLUSTERS = 3
geo_only = taxi_batch[["pickup_latitude", "pickup_longitude", "dropoff_latitude", "dropoff_longitude"]]
geo_normalized = (geo_only - geo_only.mean()) / geo_only.std()

geo_labels, geo_centers = k_means(geo_normalized, n_clusters=N_CLUSTERS)

print("--- What characterizes each geographic cluster? ---\n")
for c in range(N_CLUSTERS):
    subset = taxi_batch[geo_labels == c]
    print(f"Cluster {c} (n={len(subset)}):")
    print(f"  Avg fare: ${subset['fare_amount'].mean():.2f}")
    print(f"  Avg distance: {subset['trip_distance'].mean():.1f} mi")
    print(f"  Avg tip %: {(subset['tip_amount']/subset['total_amount']).mean()*100:.1f}%")
    print(f"  Weekend %: {subset['is_weekend'].mean()*100:.1f}%")
    print(f"  Rush hour %: {subset['is_rush_hour'].mean()*100:.1f}%")
    print(f"  Avg hour: {subset['hour'].mean():.1f}")
    print()

"""There are significant price differences! What to they correspond to geographically?"""

fig, axes = plt.subplots(int(np.ceil(N_CLUSTERS//3)), 3, figsize=(15, 10))
for c in range(N_CLUSTERS):
    if len(axes.shape) == 1:
        ax = axes[c]
    else:
        ax = axes[c // 3, c % 3]
    subset = taxi_batch[geo_labels == c]
    ax.scatter(subset["dropoff_longitude"], subset["dropoff_latitude"], alpha=0.3, s=5)
    ax.set_title(f'Cluster {c} dropoffs (n={len(subset)})')
plt.tight_layout()
plt.show()

"""There clusters correspond to:

- Upper/Mid Manhatta
- Lower Manhattan + Brooklyn + the JFK
- Outliers

So the k-means can still find some real geographical tendencies.
"""